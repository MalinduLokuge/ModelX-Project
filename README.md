# ModelX-Project ğŸ†

**ML Model for ModelX Dementia Risk Prediction Competition**

An automated machine learning system built specifically for predicting dementia risk using non-medical features.

## ğŸ‰ **Production Model: 94.34% ROC-AUC Achieved!**

**AutoGluon AutoML Model - Production Ready**
- âœ… **Validation ROC-AUC: 94.34%** (WeightedEnsemble_L4)
- âœ… **+14.87 pp improvement** over best manual model (79.47%)
- âœ… **42 models trained** with 4-level stacking
- âœ… **1,299 rows/second** inference speed
- âœ… **132 engineered features** (112 original + 20 automated)
- ğŸ“Š **See `AUTOML_TRAINING_REPORT.md` for complete details**

**Quick Use:**
```python
from autogluon.tabular import TabularPredictor

# Load production model
predictor = TabularPredictor.load('outputs/models/autogluon_optimized/')

# Make predictions
predictions = predictor.predict(new_data)
probabilities = predictor.predict_proba(new_data)
```

---

## ğŸ¯ What is This Project?

This is a **production-ready ML system** for dementia risk prediction achieving **94.34% ROC-AUC** using AutoML. The system handles the complete pipeline from raw data to trained models with explainability analysis.

**Key Features:**
- âœ… **Automated Pipeline**: One command runs everything
- âœ… **High Performance**: 94.34% ROC-AUC with AutoGluon (42 models, 4-level stacking)
- âœ… **Multiple Models**: Compare AutoML vs 8 traditional ML models
- âœ… **Explainability**: SHAP, LIME, feature importance analysis
- âœ… **Production Ready**: Complete documentation, checksums, deployment guides

## âš¡ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/MalinduLokuge/ModelX-Project.git
cd ModelX-Project

# Install dependencies
pip install -r requirements.txt
```

### ğŸš€ Run Complete Pipeline (Recommended)

**Option 1: Full Pipeline (Preprocessing + Training)**
```bash
# Run everything from scratch (~45 minutes)
python run_complete_pipeline.py
```

**Option 2: Quick Test (5 minutes)**
```bash
# Fast validation run
python run_complete_pipeline.py --quick-test
```

**Option 3: Skip Preprocessing (Training Only)**
```bash
# If data already preprocessed (~30 minutes)
python run_complete_pipeline.py --skip-preprocessing
```

### ğŸ“Š Individual Components

**If you want to run specific parts:**

```bash
# 1. Data Preprocessing Only
python run_preprocessing_simple.py

# 2. Train Manual Models Only (8 models)
python train_manual_lowmem.py

# 3. Train AutoML Model Only (42 models, 4-level stacking)
python train_autogluon_optimized.py

# 4. Generate Model Comparison Report
python model_comparison_final.py

# 5. Generate Explainability Analysis
python generate_xai_analysis.py
```

### ğŸ¯ What Each Script Does

| Script | Time | Output | Description |
|--------|------|--------|-------------|
| `run_complete_pipeline.py` | 45 min | All models + reports | **Main entry point** - runs everything |
| `run_preprocessing_simple.py` | 5 min | `data/train/`, `data/test/` | Data cleaning, splitting, balancing |
| `train_manual_lowmem.py` | 10 min | `outputs/manual_models/` | 8 traditional ML models |
| `train_autogluon_optimized.py` | 30 min | `outputs/models/autogluon_optimized/` | AutoML with 94.34% ROC-AUC |
| `model_comparison_final.py` | 2 min | `model_comparison_results/` | Compare all models |
| `generate_xai_analysis.py` | 5 min | `outputs/xai/` | Feature importance, SHAP, LIME |

### âœ… Pipeline Output

After running `run_complete_pipeline.py`, you'll get:

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/              # Preprocessed training data
â”‚   â”‚   â”œâ”€â”€ X_train_balanced.csv
â”‚   â”‚   â””â”€â”€ y_train_balanced.csv
â”‚   â””â”€â”€ test/               # Preprocessed test data
â”‚       â”œâ”€â”€ X_test.csv
â”‚       â””â”€â”€ y_test.csv
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ manual_models/      # 8 traditional ML models
â”‚   â”‚   â”œâ”€â”€ LightGBM_Tuned.pkl
â”‚   â”‚   â”œâ”€â”€ XGBoost_Tuned.pkl
â”‚   â”‚   â””â”€â”€ model_comparison.csv
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ autogluon_optimized/  # Production AutoML model (94.34% ROC-AUC)
â”‚   â””â”€â”€ xai/                # Explainability analysis
â”‚       â”œâ”€â”€ XAI_DOCUMENTATION.md
â”‚       â””â”€â”€ *.png (visualizations)
â”‚
â”œâ”€â”€ model_comparison_results/  # Model comparison reports
â”‚   â”œâ”€â”€ roc_curves_test.png
â”‚   â”œâ”€â”€ confusion_matrices_test.png
â”‚   â””â”€â”€ metrics_comparison_test.png
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ MODEL_README.md              # Complete model documentation
    â”œâ”€â”€ AUTOML_TRAINING_REPORT.md    # Training details
    â”œâ”€â”€ USAGE_SNIPPETS.md            # Code examples
    â””â”€â”€ CHECKSUMS.md                 # Model verification
```

## ğŸ“ Project Structure

```
CompeteML/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Core system (orchestrator, config, logger)
â”‚   â”œâ”€â”€ preprocessing/     # Data cleaning & preprocessing
â”‚   â”œâ”€â”€ modeling/          # AutoML training (AutoGluon)
â”‚   â”œâ”€â”€ evaluation/        # Metrics & evaluation
â”‚   â””â”€â”€ reporting/         # Submission & report generation
â”‚
â”œâ”€â”€ configs/               # Configuration presets
â”‚   â”œâ”€â”€ default.yaml      # 1-hour balanced run
â”‚   â”œâ”€â”€ competition.yaml  # 2-hour high-performance
â”‚   â””â”€â”€ quick_test.yaml   # 5-minute test
â”‚
â”œâ”€â”€ data/                 # Your datasets
â”‚   â”œâ”€â”€ raw/              # Original competition data
â”‚   â””â”€â”€ sample/           # Sample datasets for testing
â”‚
â”œâ”€â”€ outputs/              # All results
â”‚   â””â”€â”€ <run_id>/
â”‚       â”œâ”€â”€ submissions/  # Competition submission files
â”‚       â”œâ”€â”€ models/       # Trained models
â”‚       â”œâ”€â”€ recipe.txt    # What was done
â”‚       â””â”€â”€ logs/         # Execution logs
â”‚
â”œâ”€â”€ main.py              # CLI entry point
â””â”€â”€ requirements.txt     # Dependencies
```

## ğŸ® Usage

### Basic Commands

```bash
# Run pipeline
python main.py run --train <train.csv> --test <test.csv>

# Quick exploration
python main.py explore --train <train.csv>

# System info
python main.py info
```

### Advanced Options

```bash
python main.py run \
  --train data/train.csv \
  --test data/test.csv \
  --target price \
  --id-col id \
  --time-limit 7200 \
  --output-dir my_results \
  --preset competition
```

### Using Custom Config

```bash
# Create custom config
cp configs/default.yaml my_config.yaml
# Edit my_config.yaml...

# Run with custom config
python main.py run --train data/train.csv --config my_config.yaml
```

## ğŸ”§ Configuration Presets

### Quick Test (5 minutes)
- Fast preprocessing only
- No feature engineering
- Quick model training
- Perfect for testing

```bash
--preset quick
```

### Default (1 hour)
- Full preprocessing
- Auto feature engineering
- Medium quality models
- Balanced speed/performance

```bash
--preset default
```

### Competition (2 hours)
- Full preprocessing
- Advanced feature engineering
- Best quality models (8-fold bagging, 2-level stacking)
- Maximum performance

```bash
--preset competition
```

## ğŸ¤– What CompeteML Does Automatically

### 1. Smart Data Loading
- Auto-detects file format (CSV, Excel, Parquet, JSON)
- Auto-detects target column
- Auto-detects problem type (classification/regression)
- Auto-detects ID columns

### 2. Data Preprocessing
- **Missing Values**: Intelligent imputation based on missing %
- **Encoding**: Auto-selects encoding strategy by cardinality
  - Low cardinality (â‰¤10): One-hot encoding
  - Medium (â‰¤50): Label encoding
  - High (>50): Target encoding
- **Scaling**: Auto-selects scaler based on distribution
  - Outliers present: RobustScaler
  - Bounded [0,1]: MinMaxScaler
  - Default: StandardScaler

### 3. Feature Engineering (Optional)
- Interaction features
- Polynomial features
- Time-based features
- Text features (TF-IDF)
- Feature selection

### 4. Model Training
- **Primary**: AutoGluon (state-of-the-art AutoML)
- Multi-layer stacking
- Bagging for stability
- Automatic hyperparameter tuning

### 5. Outputs
- Competition submission file (ready to upload)
- Trained models (saved for later use)
- Recipe file (showing exactly what was done)
- Execution logs

## ğŸ“Š Output Files

After running, check `outputs/<run_id>/`:

```
outputs/20250114_143022/
â”œâ”€â”€ submissions/
â”‚   â””â”€â”€ submission_20250114_143022.csv    # Ready to upload!
â”œâ”€â”€ ag_models/                            # Trained models
â”œâ”€â”€ recipe.txt                            # What was done
â””â”€â”€ logs/
    â””â”€â”€ competeml_20250114_143022.log    # Detailed logs
```

## ğŸ… Competition Workflow

**Standard Competition Workflow:**

```bash
# 1. Quick test (5 min) - verify everything works
python main.py run --train train.csv --test test.csv --preset quick

# 2. Default run (1 hour) - get baseline
python main.py run --train train.csv --test test.csv

# 3. Competition run (2+ hours) - maximize performance
python main.py run --train train.csv --test test.csv --preset competition

# 4. Submit outputs/latest/submissions/submission_*.csv to competition
```

## ğŸ”¬ Understanding Your Results

### Recipe File
Shows exactly what the system did:
```
================================================================================
COMPETEML PIPELINE RECIPE
================================================================================
Run ID: 20250114_143022
Date: 2025-01-14 14:30:22

STEPS PERFORMED:
--------------------------------------------------------------------------------
1. Loaded data and detected problem type
2. Validated data quality
3. Preprocessed data (handled missing, encoded, scaled)
4. Trained models using autogluon
5. Evaluated models and generated predictions
6. Created submission file: submission_20250114_143022.csv
================================================================================
```

### Logs
Check detailed logs in `outputs/<run_id>/logs/` for:
- Data statistics
- Preprocessing decisions
- Model performance
- Feature importance
- Warnings and errors

## ğŸ“ Learning Mode

Want to learn what works? CompeteML shows you:

1. **Recipe files**: Exactly what was done
2. **Logs**: Why decisions were made
3. **Model leaderboard**: Which models performed best
4. **Feature importance**: Which features matter
5. **Code templates**: Manual implementation templates
6. **Deployment package**: Ready-to-deploy model + inference script

### Manual Mode Templates

Found in `templates/code/`:
- `preprocessing_template.py` - Replicate preprocessing manually
- `feature_engineering_template.py` - Create features manually
- `training_template.py` - Train models manually

Each template shows what auto mode did and how to replicate it.

### Utility Scripts

Found in `scripts/`:
- `setup.bat` - One-time setup
- `quick_test.bat <data>` - 5-minute validation
- `competition_run.bat <train> <test>` - Full competition run

### Deployment Package

After training, find in `outputs/<run_id>/model_<run_id>/`:
- `model.pkl` - Trained model
- `preprocessor.pkl` - Preprocessing pipeline
- `feature_engineer.pkl` - Feature engineering pipeline
- `inference.py` - Ready-to-use prediction script
- `metadata.json` - Model info and metrics

Use for deployment:
```python
from outputs.model_20250114_143022.inference import ModelPredictor

predictor = ModelPredictor('outputs/model_20250114_143022')
predictions = predictor.predict(new_data)
```

Review these to understand the automated process, then switch to manual mode when needed.

## ğŸ› ï¸ Requirements

### Core
- Python 3.8+
- pandas, numpy, scikit-learn
- AutoGluon (primary AutoML)

### Optional
- Optuna (hyperparameter tuning)
- PyCaret (backup AutoML)
- ydata-profiling (EDA reports)

Install all:
```bash
pip install -r requirements.txt
```

## ğŸ“– Configuration Options

Key configuration options (edit `configs/*.yaml`):

```yaml
# Time
time_limit: 3600  # seconds

# Preprocessing
handle_missing: true
handle_outliers: true
scaling_strategy: auto  # auto, standard, minmax, robust, none
encoding_strategy: auto  # auto, onehot, target, ordinal

# Feature Engineering
auto_features: true
interaction_features: true
polynomial_features: false

# Modeling
automl_framework: autogluon
ag_preset: medium_quality  # best_quality, high_quality, medium_quality
ag_num_bag_folds: 5
ag_num_stack_levels: 1

# Output
generate_submission: true
generate_recipe: true
```

## ğŸš€ Tips for Winning

1. **Start with quick test**: Verify everything works (5 min)
2. **Run default mode**: Get baseline (1 hour)
3. **Check recipe & logs**: Understand what worked
4. **Run competition mode**: Maximize performance (2+ hours)
5. **Submit and iterate**: Use feedback to improve

## ğŸ“ License

MIT License - feel free to use for competitions and learning!

## ğŸ¤ Contributing

Contributions welcome! This is a learning project designed to help people win ML competitions.

## ğŸ“§ Support

- Check logs in `outputs/<run_id>/logs/`
- Review recipe in `outputs/<run_id>/recipe.txt`
- Open an issue with your question

---

**Built for the ModelX dementia risk prediction competition.** ğŸ†
