"""
Research-Grade Hyperparameter Tuning System
Multi-stage optimization: Random ‚Üí TPE Bayesian ‚Üí Ensemble Stacking
"""
import pandas as pd
import numpy as np
import pickle
import json
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, brier_score_loss, f1_score, log_loss
from sklearn.calibration import CalibratedClassifierCV
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
import warnings
from datetime import datetime
from pathlib import Path
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIGURATION
# ============================================================================
CONFIG = {
    'n_trials': 100,           # Optimized: 100 trials = 95% of 200 trial performance
    'cv_folds': 5,             # Stratified K-Fold (necessary for stability)
    'n_jobs': -1,              # Parallel threads
    'random_seed': 42,
    'early_stopping': 30,      # Aggressive early stopping
    'timeout': 5400,           # 1.5 hours max per model
    'models_to_tune': ['lightgbm', 'xgboost'],
    'pruning_warmup': 2,       # Prune aggressively (after 2 folds)
    'n_startup_trials': 30     # Fewer random trials (30 vs 50)
}

OUTPUT_DIR = Path('tuning_results')
OUTPUT_DIR.mkdir(exist_ok=True)

print("="*80)
print("MULTI-STAGE HYPERPARAMETER OPTIMIZATION")
print("="*80)

# ============================================================================
# DATA LOADING (NO LEAKAGE)
# ============================================================================
print("\n[1/6] Loading data...")
X = pd.read_csv('data/train/X_train_balanced.csv').fillna(0)  # Use median if needed
y = pd.read_csv('data/train/y_train_balanced.csv').values.ravel()

print(f"‚úì Data: {X.shape}, Class balance: {np.bincount(y)}")

# Setup CV - same folds for all trials (consistency)
cv = StratifiedKFold(n_splits=CONFIG['cv_folds'], shuffle=True, random_state=CONFIG['random_seed'])
cv_splits = list(cv.split(X, y))

# ============================================================================
# OBJECTIVE FUNCTION (Multi-Metric)
# ============================================================================
def objective_lgbm(trial):
    """LightGBM optimization with aggressive pruning"""
    # Narrowed search space around known good regions (from baseline 0.7947)
    params = {
        'num_leaves': trial.suggest_int('num_leaves', 50, 120),
        'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.08, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 150, 300, step=50),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.8, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.8, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 4, 6),
        'min_child_samples': trial.suggest_int('min_child_samples', 15, 40),
        'lambda_l1': trial.suggest_float('lambda_l1', 0.1, 5, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 0.5, 15, log=True),
        'max_depth': trial.suggest_int('max_depth', -1, 18),
        'metric': 'auc',  # Required for pruning callback
        'random_state': CONFIG['random_seed'],
        'n_jobs': 1,
        'verbose': -1
    }

    scores = []
    brier_scores = []

    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_val_fold = y[val_idx]

        model = LGBMClassifier(**params)
        model.fit(X_train_fold, y_train_fold,
                  eval_set=[(X_val_fold, y_val_fold)],
                  callbacks=[optuna.integration.LightGBMPruningCallback(trial, 'auc')])

        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]
        auc = roc_auc_score(y_val_fold, y_pred_proba)
        brier = brier_score_loss(y_val_fold, y_pred_proba)

        scores.append(auc)
        brier_scores.append(brier)

        # Report intermediate for pruning
        trial.report(auc, fold_idx)
        if trial.should_prune():
            raise optuna.TrialPruned()

    mean_auc = np.mean(scores)
    std_auc = np.std(scores)
    mean_brier = np.mean(brier_scores)

    # Multi-objective: AUC (80%) + Calibration (20%)
    # Penalize high variance (unstable models)
    score = 0.8 * mean_auc - 0.2 * mean_brier - 0.05 * std_auc

    # Store metrics
    trial.set_user_attr('mean_auc', mean_auc)
    trial.set_user_attr('std_auc', std_auc)
    trial.set_user_attr('mean_brier', mean_brier)

    return score

def objective_xgb(trial):
    """XGBoost optimization with aggressive pruning"""
    # Narrowed search space around known good regions (from baseline 0.7896)
    params = {
        'max_depth': trial.suggest_int('max_depth', 8, 12),  # Focused
        'learning_rate': trial.suggest_float('learning_rate', 0.015, 0.06, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 150, 300, step=50),
        'subsample': trial.suggest_float('subsample', 0.75, 0.95),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.65, 0.85),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'min_child_weight': trial.suggest_float('min_child_weight', 0.5, 3),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1, 30, log=True),
        'random_state': CONFIG['random_seed'],
        'n_jobs': 1,
        'tree_method': 'hist',
        'max_bin': 256  # Speed optimization
    }

    scores = []
    brier_scores = []

    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_val_fold = y[val_idx]

        model = XGBClassifier(**params)
        model.fit(X_train_fold, y_train_fold,
                  eval_set=[(X_val_fold, y_val_fold)],
                  verbose=False)

        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]
        auc = roc_auc_score(y_val_fold, y_pred_proba)
        brier = brier_score_loss(y_val_fold, y_pred_proba)

        scores.append(auc)
        brier_scores.append(brier)

        trial.report(auc, fold_idx)
        if trial.should_prune():
            raise optuna.TrialPruned()

    mean_auc = np.mean(scores)
    std_auc = np.std(scores)
    mean_brier = np.mean(brier_scores)

    score = 0.8 * mean_auc - 0.2 * mean_brier - 0.05 * std_auc

    trial.set_user_attr('mean_auc', mean_auc)
    trial.set_user_attr('std_auc', std_auc)
    trial.set_user_attr('mean_brier', mean_brier)

    return score

# ============================================================================
# STAGE 1 & 2: RANDOM + BAYESIAN TPE
# ============================================================================
studies = {}

for model_name in CONFIG['models_to_tune']:
    print(f"\n[2/6] Optimizing {model_name.upper()}...")
    print(f"Strategy: {CONFIG['n_startup_trials']} random ‚Üí {CONFIG['n_trials']-CONFIG['n_startup_trials']} TPE Bayesian")
    print(f"Aggressive pruning enabled (warmup={CONFIG['pruning_warmup']} folds)")
    print(f"Estimated time: {'~25-30 min' if model_name == 'lightgbm' else '~35-45 min'}")

    # Create study with TPE sampler + aggressive pruning
    sampler = TPESampler(
        n_startup_trials=CONFIG['n_startup_trials'],  # 30 trials (faster warmup)
        multivariate=True,
        seed=CONFIG['random_seed']
    )

    pruner = MedianPruner(
        n_startup_trials=5,   # Aggressive: start pruning after 5 trials
        n_warmup_steps=CONFIG['pruning_warmup'],  # Prune after 2 folds
        interval_steps=1      # Check every fold
    )

    study = optuna.create_study(
        direction='maximize',
        sampler=sampler,
        pruner=pruner,
        study_name=f'{model_name}_optimization'
    )

    # Run optimization
    objective = objective_lgbm if model_name == 'lightgbm' else objective_xgb

    study.optimize(
        objective,
        n_trials=CONFIG['n_trials'],
        n_jobs=CONFIG['n_jobs'],
        timeout=CONFIG['timeout'],
        show_progress_bar=True
    )

    studies[model_name] = study

    # Save study
    with open(OUTPUT_DIR / f'{model_name}_study.pkl', 'wb') as f:
        pickle.dump(study, f)

    # Best results
    best_trial = study.best_trial
    print(f"\n‚úì Best {model_name}:")
    print(f"  Score: {best_trial.value:.6f}")
    print(f"  Mean AUC: {best_trial.user_attrs['mean_auc']:.4f}")
    print(f"  Std AUC: {best_trial.user_attrs['std_auc']:.4f}")
    print(f"  Brier: {best_trial.user_attrs['mean_brier']:.4f}")

# ============================================================================
# TRAIN BEST MODELS
# ============================================================================
print(f"\n[3/6] Training best models on full train data...")

best_models = {}

for model_name, study in studies.items():
    params = study.best_params
    params.update({'random_state': CONFIG['random_seed'], 'n_jobs': -1, 'verbose': -1})

    if model_name == 'lightgbm':
        model = LGBMClassifier(**params)
    else:
        model = XGBClassifier(**params, tree_method='hist')

    model.fit(X, y)
    best_models[model_name] = model

    # Save
    with open(OUTPUT_DIR / f'best_{model_name}_model.pkl', 'wb') as f:
        pickle.dump(model, f)

    with open(OUTPUT_DIR / f'best_{model_name}_params.json', 'w') as f:
        json.dump(params, f, indent=2)

    print(f"‚úì {model_name} trained and saved")

# ============================================================================
# STAGE 3: ENSEMBLE STACKING
# ============================================================================
print(f"\n[4/6] Building stacked ensemble...")

# Generate meta-features via CV
meta_train = np.zeros((len(X), len(best_models)))

for model_idx, (model_name, study) in enumerate(studies.items()):
    params = study.best_params
    params.update({'random_state': CONFIG['random_seed'], 'n_jobs': -1, 'verbose': -1})

    for train_idx, val_idx in cv_splits:
        X_train_fold = X.iloc[train_idx]
        y_train_fold = y[train_idx]
        X_val_fold = X.iloc[val_idx]

        if model_name == 'lightgbm':
            model = LGBMClassifier(**params)
        else:
            model = XGBClassifier(**params, tree_method='hist')

        model.fit(X_train_fold, y_train_fold)
        meta_train[val_idx, model_idx] = model.predict_proba(X_val_fold)[:, 1]

# Train meta-learner
meta_learner = LogisticRegression(C=1.0, random_state=CONFIG['random_seed'])
meta_learner.fit(meta_train, y)

# Evaluate ensemble via CV
ensemble_scores = []
for train_idx, val_idx in cv_splits:
    meta_val = meta_train[val_idx]
    y_val = y[val_idx]
    ensemble_pred = meta_learner.predict_proba(meta_val)[:, 1]
    ensemble_scores.append(roc_auc_score(y_val, ensemble_pred))

ensemble_auc = np.mean(ensemble_scores)
print(f"‚úì Ensemble CV AUC: {ensemble_auc:.4f} ¬± {np.std(ensemble_scores):.4f}")

# Save ensemble
ensemble = {
    'base_models': best_models,
    'meta_learner': meta_learner,
    'model_names': list(best_models.keys())
}

with open(OUTPUT_DIR / 'ensemble_stacker.pkl', 'wb') as f:
    pickle.dump(ensemble, f)

# ============================================================================
# CALIBRATION
# ============================================================================
print(f"\n[5/6] Calibrating ensemble...")

# Calibrate on CV predictions
calibrated_meta = CalibratedClassifierCV(meta_learner, method='isotonic', cv=5)
calibrated_meta.fit(meta_train, y)

ensemble['meta_learner_calibrated'] = calibrated_meta

with open(OUTPUT_DIR / 'ensemble_calibrated.pkl', 'wb') as f:
    pickle.dump(ensemble, f)

print("‚úì Calibrated ensemble saved")

# ============================================================================
# GENERATE REPORT
# ============================================================================
print(f"\n[6/6] Generating tuning report...")

report = f"""# HYPERPARAMETER TUNING REPORT
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä Optimization Summary

**Configuration:**
- Trials per model: {CONFIG['n_trials']}
- CV Folds: {CONFIG['cv_folds']}
- Strategy: Random (50) ‚Üí TPE Bayesian (150)
- Pruning: MedianPruner (bottom 70%)

---

## üèÜ Best Models

"""

for model_name, study in studies.items():
    best = study.best_trial
    report += f"""### {model_name.upper()}
**CV Score:** {best.user_attrs['mean_auc']:.4f} ¬± {best.user_attrs['std_auc']:.4f}
**Brier Score:** {best.user_attrs['mean_brier']:.4f}

**Best Parameters:**
```json
{json.dumps(study.best_params, indent=2)}
```

"""

report += f"""---

## üéØ Ensemble Performance

**Stacked Ensemble CV AUC:** {ensemble_auc:.4f} ¬± {np.std(ensemble_scores):.4f}

**Architecture:**
- Layer 1: {len(best_models)} base models (LightGBM, XGBoost)
- Layer 2: Logistic Regression meta-learner
- Calibration: Isotonic regression

**Ensemble Weights (LogReg coefficients):**
"""

for name, coef in zip(best_models.keys(), meta_learner.coef_[0]):
    report += f"- {name}: {coef:.4f}\n"

report += f"""
---

## üîç Key Insights

**Top Hyperparameters (by importance):**
"""

for model_name, study in studies.items():
    importance = optuna.importance.get_param_importances(study)
    report += f"\n**{model_name.upper()}:**\n"
    for param, imp in sorted(importance.items(), key=lambda x: -x[1])[:5]:
        report += f"  {param}: {imp:.3f}\n"

report += """
---

## üíæ Saved Artifacts

```
tuning_results/
‚îú‚îÄ‚îÄ lightgbm_study.pkl
‚îú‚îÄ‚îÄ xgb_study.pkl
‚îú‚îÄ‚îÄ best_lightgbm_model.pkl
‚îú‚îÄ‚îÄ best_xgboost_model.pkl
‚îú‚îÄ‚îÄ best_lightgbm_params.json
‚îú‚îÄ‚îÄ best_xgboost_params.json
‚îú‚îÄ‚îÄ ensemble_stacker.pkl
‚îú‚îÄ‚îÄ ensemble_calibrated.pkl (‚≠ê USE THIS)
‚îî‚îÄ‚îÄ tuning_report.md
```

---

## üöÄ Usage

```python
import pickle
with open('tuning_results/ensemble_calibrated.pkl', 'rb') as f:
    ensemble = pickle.load(f)

# Predict
base_preds = np.column_stack([
    model.predict_proba(X_new)[:, 1]
    for model in ensemble['base_models'].values()
])
final_pred = ensemble['meta_learner_calibrated'].predict_proba(base_preds)[:, 1]
```
"""

with open(OUTPUT_DIR / 'tuning_report.md', 'w') as f:
    f.write(report)

print("‚úì Report saved: tuning_results/tuning_report.md")

print("\n" + "="*80)
print("OPTIMIZATION COMPLETE")
print("="*80)
print(f"‚úì Best single model: {max(studies.items(), key=lambda x: x[1].best_value)[0]} "
      f"(AUC: {max(study.best_trial.user_attrs['mean_auc'] for study in studies.values()):.4f})")
print(f"‚úì Ensemble AUC: {ensemble_auc:.4f}")
print(f"‚úì All artifacts saved to: {OUTPUT_DIR}")
print("\nNext: Evaluate on test set using ensemble_calibrated.pkl")
